{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "from sklearn import svm, cross_validation\n",
    "import pylab as pl\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure that caffe is on the python path:\n",
    "caffe_root = '/u/zexuan/caffe/caffe/'  # this file is expected to be in {caffe_root}/examples\n",
    "caffe_real_root = '/pkgs/caffe/'\n",
    "thesis_root = '/ais/gobi2/pingpong/thesis/'\n",
    "#!ls /pkgs/caffe\n",
    "import sys\n",
    "sys.path.insert(0, caffe_real_root + 'python')\n",
    "import caffe\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "if not os.path.isfile('/u/zexuan/caffe/caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'):\n",
    "    print(\"Downloading pre-trained CaffeNet model...\")\n",
    "    !~/caffe/caffe/scripts/download_model_binary.py ~/caffe/caffe/models/bvlc_reference_caffenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MODEL_FILE = caffe_root +'models/bvlc_reference_caffenet/deploy.prototxt'\n",
    "# PRETRAINED = caffe_root +'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'\n",
    "# caffe.set_mode_cpu()\n",
    "# net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
    "#                        mean=np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy').mean(1).mean(1),\n",
    "#                        channel_swap=(2,1,0),\n",
    "#                        raw_scale=255,\n",
    "#                        image_dims=(600, 600))\n",
    "# print net.blobs['data'].data.shape\n",
    "# [(k, v.data.shape) for k, v in net.blobs.items()]\n",
    "\n",
    "\n",
    "# caffe.set_mode_cpu()\n",
    "print caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt'\n",
    "#!ls caffe_root+'models/bvlc_reference_caffenet/deploy.prototxt'\n",
    "# net = caffe.Net(caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt',\n",
    "#                 caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel',\n",
    "#                 caffe.TEST)\n",
    "\n",
    "#~/caffe/caffe/models/finetune_flickr_style/deploy.prototxt\n",
    "net = caffe.Net(caffe_root + 'models/finetune_stroke_cls/deploy.prototxt',\n",
    "                thesis_root + 'fine_tune_stroke_cls_500iter.caffemodel',\n",
    "                caffe.TEST)\n",
    "\n",
    "# input preprocessing: 'data' is the name of the input blob == net.inputs[0]\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2,0,1))\n",
    "transformer.set_mean('data', np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy').mean(1).mean(1)) # mean pixel\n",
    "transformer.set_raw_scale('data', 255)  # the reference model operates on images in [0,255] range instead of [0,1]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # the reference model has channels in BGR order instead of RGB\n",
    "print net.blobs['data'].data.shape\n",
    "[(k, v.data.shape) for k, v in net.blobs.items()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.io\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "training_set = {}\n",
    "training_set['target'] = []\n",
    "container_path = 'labeled_img_selected_all_classes/'\n",
    "\n",
    "folders = [f for f in sorted(os.listdir(container_path)) if os.path.isdir(os.path.join(container_path, f))]\n",
    "print 'folders: ', folders\n",
    "\n",
    "result = []\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(container_path, folder)\n",
    "    documents = [os.path.join(folder_path, d) for d in sorted(os.listdir(folder_path))]\n",
    "    input_10_imgs = []\n",
    "    for pic in documents:\n",
    "        if not pic.endswith('png'):\n",
    "            continue\n",
    "        img = skimage.img_as_float(skimage.io.imread(pic)).astype(np.float32)\n",
    "        input_10_imgs.append(img)\n",
    "        if len(input_10_imgs) >= 10:\n",
    "            #net.blobs['data'].data[...] = transformer.preprocess('data', training_set['data'][0])\n",
    "            #net.blobs['data'].data[...] = map(lambda x: transformer.preprocess('data', x), training_set['data'][i:i+10])\n",
    "            net.blobs['data'].data[...] = map(lambda x: transformer.preprocess('data', x), input_10_imgs)\n",
    "            out = net.forward()\n",
    "            result.append(np.mean(net.blobs['fc6'].data, axis=0))\n",
    "            input_10_imgs = []\n",
    "        training_set['target'].append(folder)\n",
    "#training_set['data']=data\n",
    "#training_set['target']=target\n",
    "print 'Total time loading training data: ', time.time()-start_time, ' seconds'\n",
    "print 'number of images: ', len(training_set['target'])\n",
    "#print 'dmensionality of each image: ', input_10_imgs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "testing_set = {}\n",
    "testing_set['target'] = []\n",
    "container_path = 'labeled_img_selected_all_classes_testset/'\n",
    "\n",
    "folders = [f for f in sorted(os.listdir(container_path)) if os.path.isdir(os.path.join(container_path, f))]\n",
    "print 'folders: ', folders\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(container_path, folder)\n",
    "    documents = [os.path.join(folder_path, d) for d in sorted(os.listdir(folder_path))]\n",
    "    input_10_imgs = []\n",
    "    for pic in documents:\n",
    "        if not pic.endswith('png'):\n",
    "            continue\n",
    "        img = skimage.img_as_float(skimage.io.imread(pic)).astype(np.float32)\n",
    "        input_10_imgs.append(img)\n",
    "        if len(input_10_imgs) >= 10:\n",
    "            #net.blobs['data'].data[...] = transformer.preprocess('data', training_set['data'][0])\n",
    "            #net.blobs['data'].data[...] = map(lambda x: transformer.preprocess('data', x), training_set['data'][i:i+10])\n",
    "            net.blobs['data'].data[...] = map(lambda x: transformer.preprocess('data', x), input_10_imgs)\n",
    "            out = net.forward()\n",
    "            result.append(np.mean(net.blobs['fc6'].data, axis=0))\n",
    "            input_10_imgs = []\n",
    "        testing_set['target'].append(folder)\n",
    "#training_set['data']=data\n",
    "#training_set['target']=target\n",
    "print 'Total time loading training data: ', time.time()-start_time, ' seconds'\n",
    "print 'number of images: ', len(testing_set['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print caffe.io.load_image(thesis_root + 'labeled_image_selected/bottom_player_winning_selected/point_00001_frame_00033.png').shape\n",
    "# print caffe.io.load_image(thesis_root + 'labeled_image_selected/bottom_player_winning_selected/point_00001_frame_00033.png')[0][0]\n",
    "# print training_set['data'][0][0][0]\n",
    "\n",
    "'''\n",
    "result = []\n",
    "#for i in xrange(2200, 2300,10):\n",
    "for i in xrange(0, traning_set_size, 10):\n",
    "    #net.blobs['data'].data[...] = transformer.preprocess('data', training_set['data'][0])\n",
    "    net.blobs['data'].data[...] = map(lambda x: transformer.preprocess('data', x), training_set['data'][i:i+10])\n",
    "    out = net.forward()\n",
    "    #print net.blobs['fc7'].data.shape\n",
    "    result.append(np.mean(net.blobs['fc6'].data, axis=0))\n",
    "'''\n",
    "\n",
    "print 'Number of input to SVM: ', len(result), 'Size of each input', result[0].shape\n",
    "\n",
    "print transformer.deprocess('data', net.blobs['data'].data[0]).shape\n",
    "print 'Sample image from the training set'\n",
    "#plt.imshow(transformer.deprocess('data', net.blobs['data'].data[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(result, open(\"nn_result_fc6_unnormalized_clf_7videos.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import normalize\n",
    "print len(result)\n",
    "print np.max(result[0])\n",
    "print result[0].shape\n",
    "normalized_result = []\n",
    "#normalized_result = normalize(result)\n",
    "for r in result:\n",
    "    #normalized_result.append(r/np.max(r))\n",
    "    normalized_result.append((r-np.mean(r))/np.std(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(normalized_result, open(\"nn_result_fc6_normalized_clf_7videos.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(normalized_result)\n",
    "print np.max(normalized_result[0])\n",
    "print normalized_result[0].shape\n",
    "print result[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "traning_set_size = len(training_set['target'])\n",
    "testing_set_size = len(testing_set['target'])\n",
    "print 'traning_set_size: ', traning_set_size\n",
    "print 'normalized_result size: ', len(normalized_result)\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(result, training_set['target'][0:traning_set_size:10], test_size=0.25, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(normalized_result, training_set['target'][0:traning_set_size:10], test_size=0.15, random_state=2)\n",
    "X_train = normalized_result[:traning_set_size/10]\n",
    "X_test = normalized_result[traning_set_size/10:]\n",
    "y_train = training_set['target'][0:traning_set_size:10]\n",
    "y_test = testing_set['target'][0:traning_set_size:10]\n",
    "\n",
    "\n",
    "print 'X_train: %d; X_test: %d; y_train: %d; y_test: %d'%(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, cross_validation\n",
    "X_train = np.asarray(X_train)\n",
    "svc = svm.SVC(kernel='precomputed')\n",
    "kernel_train = np.dot(X_train, X_train.T)  # linear kernel\n",
    "svc.fit(kernel_train, y_train)\n",
    "\n",
    "# Testing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "kernel_test = np.dot(X_test, X_train.T)\n",
    "y_pred = svc.predict(kernel_test)\n",
    "print accuracy_score(y_test, y_pred)\n",
    "\n",
    "import pandas as pd\n",
    "y_true = pd.Series(y_test)\n",
    "y_pred = pd.Series(y_pred)\n",
    "\n",
    "#print pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "print confusion_matrix(y_test, y_pred).shape\n",
    "print confusion_matrix(y_test, y_pred)\n",
    "#print '\\n'.join(sorted(list(set(y_test))))\n",
    "print len(set(y_test).union(set(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in [0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 500]:\n",
    "    svc = svm.SVC(kernel='linear', C=c) \n",
    "    kfold = cross_validation.KFold(len(X_train), n_folds=4, shuffle=True)\n",
    "    cv_result = cross_validation.cross_val_score(svc, X_train, y_train, cv=kfold, n_jobs=1)\n",
    "    print 'C = %g, Cross validation: %s'%(c, cv_result)\n",
    "    print 'C = %g, Mean value: %g'%(c, np.mean(cv_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in [0.01, 0.05, 0.1, 1, 10, 100, 1000, 1000000]:\n",
    "    svc = svm.SVC(C=c, kernel='poly', degree=1)\n",
    "    kfold = cross_validation.KFold(len(X_train), n_folds=4, shuffle=True)\n",
    "    cv_result = cross_validation.cross_val_score(svc, X_train, y_train, cv=kfold, n_jobs=1)\n",
    "    print 'C = %g, Cross validation: %s'%(c, cv_result)\n",
    "    print 'C = %g, Mean value: %g'%(c, np.mean(cv_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in [0.01, 0.05, 0.1, 1, 10, 100, 1000, 1000000]:\n",
    "    svc = svm.SVC(C=c, kernel='poly', degree=2)\n",
    "    kfold = cross_validation.KFold(len(X_train), n_folds=4, shuffle=True)\n",
    "    cv_result = cross_validation.cross_val_score(svc, X_train, y_train, cv=kfold, n_jobs=1)\n",
    "    print 'C = %g, Cross validation: %s'%(c, cv_result)\n",
    "    print 'C = %g, Mean value: %g'%(c, np.mean(cv_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in [0.01, 0.05, 0.1, 1, 10, 100, 1000, 1000000]:\n",
    "    svc = svm.SVC(C=c, kernel='poly', degree=3)\n",
    "    kfold = cross_validation.KFold(len(X_train), n_folds=4, shuffle=True)\n",
    "    cv_result = cross_validation.cross_val_score(svc, X_train, y_train, cv=kfold, n_jobs=1)\n",
    "    print 'C = %g, Cross validation: %s'%(c, cv_result)\n",
    "    print 'C = %g, Mean value: %g'%(c, np.mean(cv_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_best = svm.SVC(kernel='linear', C=0.01).fit(X_train, y_train)\n",
    "clf_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(X_test)\n",
    "print len(y_test)\n",
    "for i,_ in enumerate(X_test):\n",
    "    if clf_best.predict(X_test[i])[0] == y_test[i]:\n",
    "        print 'SAME: ', clf_best.predict(X_test[i])[0]\n",
    "    else:\n",
    "        print 'DIFFERENT: ', clf_best.predict(X_test[i])[0], y_test[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "kfold = cross_validation.KFold(len(X_train), n_folds=4, shuffle=True)\n",
    "cv_result = cross_validation.cross_val_score(neigh, X_train, y_train, cv=kfold, n_jobs=1)\n",
    "\n",
    "print 'Cross validataion result', cv_result, '. Avg:', np.mean(cv_result)\n",
    "    \n",
    "clf = neigh.fit(X_train, y_train)\n",
    "print 'Testing result', clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "cv_result = cross_validation.cross_val_score(gnb, X_train, y_train, cv=kfold, n_jobs=1)\n",
    "print 'Cross validataion result', cv_result, '. Avg:', np.mean(cv_result)\n",
    "\n",
    "clf = gnb.fit(X_train, y_train)\n",
    "print 'Testing result', clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "cv_result = cross_validation.cross_val_score(lr, X_train, y_train, cv=kfold, n_jobs=1)\n",
    "print 'Cross validataion result', cv_result, '. Avg:', np.mean(cv_result)\n",
    "\n",
    "clf = lr.fit(X_train, y_train)\n",
    "print 'Testing result', clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(clf, open(\"fc6_normalized_lr_clf_7videos_2.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sorted(list(set(y_train)))\n",
    "print len(sorted(list(set(y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
